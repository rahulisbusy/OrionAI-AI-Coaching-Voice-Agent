# ğŸŒŒ OrionAI

OrionAI is a cutting-edge conversational AI assistant that brings together real-time voice transcription, intelligent responses, and natural-sounding voice output â€” all packed into a sleek Next.js app.

## ğŸš€ Features

- ğŸ™ï¸ **Real-Time Speech Recognition** â€” Powered by [Deepgram](https://deepgram.com/), OrionAI listens and transcribes speech in real-time.
- ğŸ§  **Conversational Intelligence** â€” Responds intelligently to queries using [Google Gemini](https://deepmind.google/technologies/gemini/).
- ğŸ”Š **Natural Voice Output** â€” Converts AI responses into lifelike speech using [ElevenLabs](https://www.elevenlabs.io/).
- âš¡ **Fast, Responsive UI** â€” Built with Next.js and Tailwind CSS for a modern, responsive user experience.
- ğŸŒ **Streaming Architecture** â€” No delay between voice input and AI response.
- ğŸ§© **Modular & Extensible** â€” Easy to integrate more LLMs, voice engines, or features.

## ğŸ› ï¸ Tech Stack

- **Frontend:** Next.js, React, Tailwind CSS  
- **Speech-to-Text:** Deepgram SDK (WebSocket streaming)  
- **AI Engine:** Google Gemini API  
- **Text-to-Speech:** ElevenLabs API  
- **State Management:** React Hooks + Context API  
- **Deployment:** Vercel / Custom server (optional)

## ğŸ“¸ Demo

> _Coming soon: Live demo and walkthrough video._

## ğŸ§° Getting Started

### 1. Clone the Repo

```bash
git clone https://github.com/yourusername/orionai.git
cd orionai
```

### 2. Install Dependencies

```bash
npm install
```

### 3. Setup Environment Variables

Create a `.env.local` file in the root and add:

```env
NEXT_PUBLIC_DEEPGRAM_API_KEY=your_deepgram_key
GEMINI_API_KEY=your_google_gemini_key
ELEVENLABS_API_KEY=your_elevenlabs_key
ELEVENLABS_VOICE_ID=your_elevenlabs_voice_id
```

### 4. Run the Development Server

```bash
npm run dev
```

Visit `http://localhost:3000` to see the app in action.

## ğŸ§ª How It Works

1. **User speaks** â†’ microphone stream is transcribed live via Deepgram WebSocket.  
2. **Transcript is sent to Gemini** â†’ the AI generates a smart response.  
3. **AI response is converted to speech** â†’ via ElevenLabsâ€™ lifelike voice synthesis.  
4. **Result is streamed back** to the user for a real-time, voice-powered interaction.

## ğŸ“¦ Build for Production

```bash
npm run build
npm start
```

## ğŸ§© Future Features

- ğŸ¯ Voice personalization and user memory  
- ğŸ”„ Multi-turn conversation support  
- ğŸ–¼ï¸ Vision and image input (Gemini Pro Vision)  
- ğŸ“± Mobile PWA support  
- ğŸ” Auth + persistent chat history  

## ğŸ¤ Contributing

Pull requests are welcome! Please open an issue first to discuss what you would like to change or add.

## ğŸ“„ License

MIT License. See [`LICENSE`](./LICENSE) for details.

---

Made with ğŸ’™ by [Pritam Chakraborty](https://github.com/rahulisbusy)
